{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnmovedFlowV2:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize attributes.\n",
    "        \"\"\"\n",
    "        self.problem_type = None\n",
    "        self.target = None\n",
    "        self.full_dataframe = None\n",
    "        self.model1 = None\n",
    "        self.model2 = None\n",
    "        self.model3 = None\n",
    "        self.Ensemble1 = None\n",
    "        self.Ensemble2 = None\n",
    "        self.random_state = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "        \"\"\"\n",
    "        Imports necessary libraries globally.\n",
    "        \"\"\"\n",
    "        # Basic manipulation\n",
    "        global pd, np\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # Visualization\n",
    "        global sns, plt, px, go\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        # Creating sample data\n",
    "        global make_classification, make_regression\n",
    "        from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "        # Splitting data into training and testing sets\n",
    "        global train_test_split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # Imputing missing values\n",
    "        global SimpleImputer, KNNImputer\n",
    "        from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "        # Feature Extraction / Dimensionality reduction\n",
    "        global PCA, TSNE, umap\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.manifold import TSNE\n",
    "        import umap\n",
    "\n",
    "        # Scaling\n",
    "        global StandardScaler, MinMaxScaler\n",
    "        from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "        # Models\n",
    "        global LogisticRegression, LinearRegression\n",
    "        global RandomForestClassifier, RandomForestRegressor\n",
    "        global XGBClassifier, XGBRegressor\n",
    "        global VotingClassifier, VotingRegressor\n",
    "        global StackingClassifier, StackingRegressor\n",
    "        from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "        from xgboost import XGBClassifier, XGBRegressor\n",
    "        from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "        from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "\n",
    "        # One hot encoding and target encoding and enable any needed things\n",
    "        global OneHotEncoder, TargetEncoder\n",
    "        from category_encoders import TargetEncoder\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        # Transforms\n",
    "        global PowerTransformer, FunctionTransformer\n",
    "        from sklearn.preprocessing import PowerTransformer\n",
    "        from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "        # Outlier removal\n",
    "        global iqr, zscore, IsolationForest, LocalOutlierFactor\n",
    "        from scipy.stats import iqr, zscore\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "        # Cross-validation\n",
    "        global cross_val_score\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        # Finding optimal hyperparameters\n",
    "        global GridSearchCV\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        # Evaluation metrics (Regression)\n",
    "        global mean_squared_error, mean_absolute_error, r2_score\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "        # Evaluation metrics (Classification) including ones needed for the roc curve and info and plotting the roc curve\n",
    "        global accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, classification_report\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, classification_report\n",
    "\n",
    "        # Hide warnings\n",
    "        global warnings\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "    # Make a function that splits the full dataframe into training and testing data, and updating the training and testing data attributes\n",
    "    def split_data(self, test_size=0.2, random_state=None, stratify=None, target=None):\n",
    "        \"\"\"\n",
    "        Split the full dataframe into training and testing data.\n",
    "    \n",
    "        Parameters:\n",
    "        - test_size: The proportion of the dataset to include in the test split.\n",
    "        - random_state: The seed used by the random number generator. this should use the attribute of the object.\n",
    "        - stratify: If not None, data is split in a stratified fashion, using this as the class labels.\n",
    "        - target: The target variable to stratify by.\n",
    "        \"\"\"\n",
    "        # If a random state is not provided, use the one from the object, which is itself None by default\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "\n",
    "        # If the target variable is not provided, use the one from the object, which is itself None by default\n",
    "        if target is None:\n",
    "            target = self.target\n",
    "\n",
    "        # If stratify is None, split the data without stratification\n",
    "        if stratify is None:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.full_dataframe.drop(target, axis=1), self.full_dataframe[target], test_size=test_size, random_state=random_state)\n",
    "        # Otherwise, split the data with stratification\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.full_dataframe.drop(target, axis=1), self.full_dataframe[target], test_size=test_size, random_state=random_state, stratify=self.full_dataframe[stratify])\n",
    "\n",
    "        # Print message\n",
    "        print(\"Data split successful into attributes X_train, X_test, y_train, and y_test\")\n",
    "\n",
    "    # Make a function that generates some sample data using make_classification or make_regression while allowing the user to set the common parameters for those functions, and be sure to update the full dataframe attribute\n",
    "    def use_sample_data(self, n_samples=1000, n_features=10, n_informative=5, n_redundant=2, random_state=None):\n",
    "        \"\"\"\n",
    "        Generate sample data using make_classification or make_regression.\n",
    "    \n",
    "        Parameters:\n",
    "        - n_samples: The number of samples to generate.\n",
    "        - n_features: The number of features to generate.\n",
    "        - n_informative: The number of informative features to generate.\n",
    "        - n_redundant: The number of redundant features to generate.\n",
    "        - n_targets: The number of target variables to generate.\n",
    "        - random_state: The random state to use for reproducibility.\n",
    "        \"\"\"\n",
    "        # If a random state is not provided, use the one from the object, which is itself None by default\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "\n",
    "        # If a problem type is not provided, use classification by default\n",
    "        if self.problem_type is None:\n",
    "            self.problem_type = 'classification'\n",
    "            print(\"Note: Problem type not provided, defaulting to classification (to specify, set classification or regression in the problem_type attribute)\")\n",
    "\n",
    "        # If regression is True, generate regression data, otherwise generate classification data\n",
    "        if self.problem_type == 'regression':\n",
    "            X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, random_state=random_state)\n",
    "        else:\n",
    "            X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, random_state=random_state)\n",
    "\n",
    "        # Turn this into a dataframe while setting the object's full_dataframe and target attributes\n",
    "        self.full_dataframe = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(1, n_features+1)])\n",
    "        self.full_dataframe['target'] = y\n",
    "        self.target = 'target'\n",
    "\n",
    "    # Define a function to plot the correlation heatmap of the full dataframe and the training and testing data if they exist\n",
    "    def relationship_heatmap(self, type='correlation', annot=True, cmap='seismic', center=0, figsize_x=10, figsize_y=8):\n",
    "        \"\"\"\n",
    "        Display a heatmap of the correlation matrix, ensure each square shows a number.\n",
    "    \n",
    "        Parameters:\n",
    "        - type: The type of relationship to plot, either 'correlation' or 'covariance'.\n",
    "        - annot: If True, write the data value in each cell.\n",
    "        - cmap: The mapping from data values to color space.\n",
    "        - center: The value at which to center the colormap when plotting divergent data.\n",
    "        - figsize_x: Width of the figure.\n",
    "        - figsize_y: Height of the figure.\n",
    "        \"\"\"\n",
    "        # If the full dataframe is not none plot the correlation heatmap for the full dataframe\n",
    "        if self.full_dataframe is not None:\n",
    "            if type == 'correlation':\n",
    "                print(\"Correlation Heatmap for the full dataframe\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.full_dataframe.corr(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "            elif type == 'covariance':\n",
    "                print(\"Covariance Heatmap for the full dataframe\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.full_dataframe.cov(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "\n",
    "        # If X_train is not none plot the correlation heatmap for X_train\n",
    "        if self.X_train is not None:\n",
    "            if type == 'correlation':\n",
    "                print(\"Correlation Heatmap for X_train\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.X_train.corr(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "            elif type == 'covariance':\n",
    "                print(\"Covariance Heatmap for X_train\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.X_train.cov(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "\n",
    "        # If X_test is not none plot the correlation heatmap for X_test\n",
    "        if self.X_test is not None:\n",
    "            if type == 'correlation':\n",
    "                print(\"Correlation Heatmap for X_test\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.X_test.corr(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "            elif type == 'covariance':\n",
    "                print(\"Covariance Heatmap for X_test\")\n",
    "                plt.figure(figsize=(figsize_x, figsize_y))\n",
    "                sns.heatmap(self.X_test.cov(), annot=annot, cmap=cmap, center=center)\n",
    "                plt.show()\n",
    "\n",
    "    # Define a function for feature extraction / dimensionality reduction\n",
    "    def extract(self, method='PCA', n_components=2, custom_params={}, exclude_columns=[]):\n",
    "        \"\"\"\n",
    "        Extract the features of / reduce the dimensionality of the data.\n",
    "    \n",
    "        Parameters:\n",
    "        - method: The extraction / dimensionality reduction method to use, either 'PCA', 'TSNE', or 'UMAP'.\n",
    "        - n_components: The number of components to reduce the data to.\n",
    "        - custom_params: A dictionary of custom parameters to use.\n",
    "        - exclude_columns: A list of columns to exclude.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a PCA, TSNE, and UMAP object\n",
    "        pca_object = PCA(n_components=n_components, **custom_params)\n",
    "        tsne_object = TSNE(n_components=n_components, **custom_params)\n",
    "        umap_object = umap.UMAP(n_components=n_components, **custom_params)\n",
    "\n",
    "        # Make a temp copy of the target column if it exists\n",
    "        if self.target is not None:\n",
    "            temp_target = self.full_dataframe[self.target].copy()\n",
    "\n",
    "        # If the full dataframe is not none, extract features / reduce dimensionality\n",
    "        if self.full_dataframe is not None:\n",
    "            if self.target is None:\n",
    "                if method == 'PCA':\n",
    "                    self.full_dataframe = pd.DataFrame(pca_object.fit_transform(self.full_dataframe.drop(exclude_columns, axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "                elif method == 'TSNE':\n",
    "                    self.full_dataframe = pd.DataFrame(tsne_object.fit_transform(self.full_dataframe.drop(exclude_columns, axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "                elif method == 'UMAP':\n",
    "                    self.full_dataframe = pd.DataFrame(umap_object.fit_transform(self.full_dataframe.drop(exclude_columns, axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "            else:\n",
    "                if method == 'PCA':\n",
    "                    self.full_dataframe = pd.DataFrame(pca_object.fit_transform(self.full_dataframe.drop(exclude_columns+[self.target], axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "                elif method == 'TSNE':\n",
    "                    self.full_dataframe = pd.DataFrame(tsne_object.fit_transform(self.full_dataframe.drop(exclude_columns+[self.target], axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "                elif method == 'UMAP':\n",
    "                    self.full_dataframe = pd.DataFrame(umap_object.fit_transform(self.full_dataframe.drop(exclude_columns+[self.target], axis=1)), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "\n",
    "                # Re-attach the target column\n",
    "                self.full_dataframe[self.target] = temp_target\n",
    "\n",
    "        # If X_train is not none, extract features / reduce dimensionality\n",
    "        if self.X_train is not None:\n",
    "            if method == 'PCA':\n",
    "                self.X_train = pd.DataFrame(pca_object.fit_transform(self.X_train), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "            elif method == 'TSNE':\n",
    "                self.X_train = pd.DataFrame(tsne_object.fit_transform(self.X_train), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "            elif method == 'UMAP':\n",
    "                self.X_train = pd.DataFrame(umap_object.fit_transform(self.X_train), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "\n",
    "        # If X_test is not none, extract features / reduce dimensionality\n",
    "        if self.X_test is not None:\n",
    "            if method == 'PCA':\n",
    "                self.X_test = pd.DataFrame(pca_object.transform(self.X_test), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "            elif method == 'TSNE':\n",
    "                self.X_test = pd.DataFrame(tsne_object.transform(self.X_test), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "            elif method == 'UMAP':\n",
    "                self.X_test = pd.DataFrame(umap_object.transform(self.X_test), columns=[f\"component_{i}\" for i in range(1, n_components+1)])\n",
    "    \n",
    "    # Define a function for previewing PCA, TSNE, and UMAP info\n",
    "    def preview(self, method='PCA'):\n",
    "        \"\"\"\n",
    "        Preview the explained variances and 3D plots of the data.\n",
    "    \n",
    "        Parameters:\n",
    "        - method: The method to use, either 'PCA', 'TSNE', or 'UMAP'.\n",
    "        \"\"\"\n",
    "        # If the method id PCA, first plot the cumulative explained variances of the existing features over time, then into a preview of what a copy of the data reduced to 3D would look like when colored against the target if applicable\n",
    "        if method == 'PCA':\n",
    "            if self.full_dataframe is not None:\n",
    "                if self.target is None:\n",
    "                    pca_object = PCA()\n",
    "                    pca_object.fit(self.full_dataframe)\n",
    "                    explained_variances = pca_object.explained_variance_ratio_\n",
    "                    cumulative_explained_variances = np.cumsum(explained_variances)\n",
    "                    fig = go.Figure()\n",
    "                    fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(explained_variances)+1)], y=explained_variances, mode='lines+markers', name='Explained Variance'))\n",
    "                    fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(cumulative_explained_variances)+1)], y=cumulative_explained_variances, mode='lines+markers', name='Cumulative Explained Variance'))\n",
    "                    fig.update_layout(title='Explained Variances of the Components For the Full Dataframe', xaxis_title='Component', yaxis_title='Explained Variance')\n",
    "                    fig.show()\n",
    "                else:\n",
    "                    pca_object = PCA()\n",
    "                    pca_object.fit(self.full_dataframe.drop(self.target, axis=1))\n",
    "                    explained_variances = pca_object.explained_variance_ratio_\n",
    "                    cumulative_explained_variances = np.cumsum(explained_variances)\n",
    "                    fig = go.Figure()\n",
    "                    fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(explained_variances)+1)], y=explained_variances, mode='lines+markers', name='Explained Variance'))\n",
    "                    fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(cumulative_explained_variances)+1)], y=cumulative_explained_variances, mode='lines+markers', name='Cumulative Explained Variance'))\n",
    "                    fig.update_layout(title='Explained Variances of the Components', xaxis_title='Component', yaxis_title='Explained Variance')\n",
    "                    fig.show()\n",
    "            \n",
    "            # Now also for the X_train and X_test data if they exist\n",
    "            if self.X_train is not None:\n",
    "                pca_object = PCA()\n",
    "                pca_object.fit(self.X_train)\n",
    "                explained_variances = pca_object.explained_variance_ratio_\n",
    "                cumulative_explained_variances = np.cumsum(explained_variances)\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(explained_variances)+1)], y=explained_variances, mode='lines+markers', name='Explained Variance'))\n",
    "                fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(cumulative_explained_variances)+1)], y=cumulative_explained_variances, mode='lines+markers', name='Cumulative Explained Variance'))\n",
    "                fig.update_layout(title='Explained Variances of the Components For X_train', xaxis_title='Component', yaxis_title='Explained Variance')\n",
    "                fig.show()\n",
    "                \n",
    "            if self.X_test is not None:\n",
    "                pca_object = PCA()\n",
    "                pca_object.fit(self.X_test)\n",
    "                explained_variances = pca_object.explained_variance_ratio_\n",
    "                cumulative_explained_variances = np.cumsum(explained_variances)\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(explained_variances)+1)], y=explained_variances, mode='lines+markers', name='Explained Variance'))\n",
    "                fig.add_trace(go.Scatter(x=[f\"component_{i}\" for i in range(1, len(cumulative_explained_variances)+1)], y=cumulative_explained_variances, mode='lines+markers', name='Cumulative Explained Variance'))\n",
    "                fig.update_layout(title='Explained Variances of the Components For X_test', xaxis_title='Component', yaxis_title='Explained Variance')\n",
    "                fig.show() \n",
    "                \n",
    "            # Now for the 3D part of it\n",
    "            if self.full_dataframe is not None:\n",
    "                if self.target is not None:\n",
    "                    pca_object = PCA(n_components=3)\n",
    "                    pca_data = pca_object.fit_transform(self.full_dataframe.drop(self.target, axis=1))\n",
    "                    pca_data = pd.DataFrame(pca_data, columns=['component_1', 'component_2', 'component_3'])\n",
    "                    pca_data['target'] = self.full_dataframe[self.target]\n",
    "                    fig = px.scatter_3d(pca_data, x='component_1', y='component_2', z='component_3', color='target', title='3D PCA Plot of the Full Dataframe')\n",
    "                    fig.show()\n",
    "                else:\n",
    "                    pca_object = PCA(n_components=3)\n",
    "                    pca_data = pca_object.fit_transform(self.full_dataframe)\n",
    "                    pca_data = pd.DataFrame(pca_data, columns=['component_1', 'component_2', 'component_3'])\n",
    "                    fig = px.scatter_3d(pca_data, x='component_1', y='component_2', z='component_3', title='3D PCA Plot of the Full Dataframe')\n",
    "                    fig.show()\n",
    "                    \n",
    "            # Now also for the X_train and X_test data if they exist, while using y_train for the target coloring, make sure everything is properly aligned so I dont get grey null values on the plot\n",
    "            if self.X_train is not None:\n",
    "                pca_object = PCA(n_components=3)\n",
    "                pca_data = pca_object.fit_transform(self.X_train)\n",
    "                pca_data = pd.DataFrame(pca_data, columns=['component_1', 'component_2', 'component_3'])\n",
    "                pca_data['target'] = self.y_train\n",
    "                fig = px.scatter_3d(pca_data, x='component_1', y='component_2', z='component_3', color='target', title='3D PCA Plot of X_train')\n",
    "                fig.show()\n",
    "                \n",
    "            if self.X_test is not None:\n",
    "                pca_object = PCA(n_components=3)\n",
    "                pca_data = pca_object.fit_transform(self.X_test)\n",
    "                pca_data = pd.DataFrame(pca_data, columns=['component_1', 'component_2', 'component_3'])\n",
    "                pca_data['target'] = self.y_test\n",
    "                fig = px.scatter_3d(pca_data, x='component_1', y='component_2', z='component_3', color='target', title='3D PCA Plot of X_test')\n",
    "                fig.show()\n",
    "                \n",
    "    # Define a function for encoding with different options\n",
    "    def encode(self, method='onehot', columns=[], custom_params={}):\n",
    "        \"\"\"\n",
    "        Encode the data using one-hot or target encoding.\n",
    "    \n",
    "        Parameters:\n",
    "        - method: The encoding method to use, either 'onehot' or 'target'.\n",
    "        - columns: A list of columns to encode.\n",
    "        - custom_params: A dictionary of custom parameters to use when encoding the data.\n",
    "        \"\"\"\n",
    "        # Create a one-hot and target encoder\n",
    "        onehot_encoder = OneHotEncoder(**custom_params)\n",
    "        target_encoder = TargetEncoder(**custom_params)\n",
    "\n",
    "        # Save a list of original column names for later use\n",
    "        temp_original_columns = self.full_dataframe.columns\n",
    "\n",
    "        # If the full dataframe is not none, encode it\n",
    "        if self.full_dataframe is not None:\n",
    "            if method == 'onehot':\n",
    "                self.full_dataframe = pd.get_dummies(self.full_dataframe, columns=columns)\n",
    "            elif method == 'target':\n",
    "                self.full_dataframe.loc[:, columns] = target_encoder.fit_transform(self.full_dataframe[columns], self.full_dataframe[self.target])\n",
    "\n",
    "        # If X_train is not none, encode it\n",
    "        if self.X_train is not None:\n",
    "            if method == 'onehot':\n",
    "                self.X_train = pd.get_dummies(self.X_train, columns=columns)\n",
    "            elif method == 'target':\n",
    "                self.X_train.loc[:, columns] = target_encoder.fit_transform(self.X_train[columns], self.y_train)\n",
    "\n",
    "        # If X_test is not none, encode it\n",
    "        if self.X_test is not None:\n",
    "            if method == 'onehot':\n",
    "                self.X_test = pd.get_dummies(self.X_test, columns=columns)\n",
    "            elif method == 'target':\n",
    "                self.X_test.loc[:, columns] = target_encoder.transform(self.X_test[columns])\n",
    "\n",
    "        # Ensure if onehot is used that the new columns are integers.\n",
    "        if method == 'onehot':\n",
    "            temp_new_columns = self.full_dataframe.columns\n",
    "            temp_new_columns = [x for x in temp_new_columns if x not in temp_original_columns]\n",
    "            for col in temp_new_columns:\n",
    "                self.full_dataframe[col] = self.full_dataframe[col].astype(int)\n",
    "            if self.X_train is not None:\n",
    "                temp_new_columns = self.X_train.columns\n",
    "                temp_new_columns = [x for x in temp_new_columns if x not in temp_original_columns]\n",
    "                for col in temp_new_columns:\n",
    "                    self.X_train[col] = self.X_train[col].astype(int)\n",
    "            if self.X_test is not None:\n",
    "                temp_new_columns = self.X_test.columns\n",
    "                temp_new_columns = [x for x in temp_new_columns if x not in temp_original_columns]\n",
    "                for col in temp_new_columns:\n",
    "                    self.X_test[col] = self.X_test[col].astype(int)\n",
    "\n",
    "        # Ensure if onehot is used that the target column is moved back to the last column in the full_dataframe ahead of the new columns\n",
    "        if method == 'onehot':\n",
    "            if self.full_dataframe is not None:\n",
    "                if self.target is not None:\n",
    "                    temp_target = self.full_dataframe[self.target]\n",
    "                    self.full_dataframe.drop(self.target, axis=1, inplace=True)\n",
    "                    self.full_dataframe[self.target] = temp_target\n",
    "\n",
    "    # Define a function to check the dtypes of the full dataframe and the training and testing data if they exist\n",
    "    def dtypes(self):\n",
    "        \"\"\"\n",
    "        Check the data types of the full dataframe and the training and testing data if they exist.\n",
    "        \"\"\"\n",
    "        # If the full dataframe is not none, print its data types\n",
    "        if self.full_dataframe is not None:\n",
    "            print(\"Data Types of the Full Dataframe:\")\n",
    "            print(self.full_dataframe.dtypes)\n",
    "\n",
    "        # If X_train is not none, print its data types\n",
    "        if self.X_train is not None:\n",
    "            print(\"\\nData Types of X_train:\")\n",
    "            print(self.X_train.dtypes)\n",
    "\n",
    "        # If X_test is not none, print its data types\n",
    "        if self.X_test is not None:\n",
    "            print(\"\\nData Types of X_test:\")\n",
    "            print(self.X_test.dtypes)\n",
    "\n",
    "        # If y_train is not none, print its data types\n",
    "        if self.y_train is not None:\n",
    "            print(\"\\nData Types of y_train:\")\n",
    "            print(self.y_train.dtypes)\n",
    "\n",
    "        # If y_test is not none, print its data types\n",
    "        if self.y_test is not None:\n",
    "            print(\"\\nData Types of y_test:\")\n",
    "            print(self.y_test.dtypes)\n",
    "\n",
    "    # Define a function to check the shapes of the full dataframe and the training and testing data if they exist\n",
    "    def shapes(self):\n",
    "        \"\"\"\n",
    "        Check the shapes of the full dataframe and the training and testing data if they exist.\n",
    "        \"\"\"\n",
    "        # If the full dataframe is not none, print its shape\n",
    "        if self.full_dataframe is not None:\n",
    "            print(f\"Shape of the Full Dataframe: {self.full_dataframe.shape[0]} rows, {self.full_dataframe.shape[1]} columns\")\n",
    "\n",
    "        # If X_train is not none, print its shape\n",
    "        if self.X_train is not None:\n",
    "            print(f\"Shape of X_train: {self.X_train.shape[0]} rows, {self.X_train.shape[1]} columns\")\n",
    "\n",
    "        # If X_test is not none, print its shape\n",
    "        if self.X_test is not None:\n",
    "            print(f\"Shape of X_test: {self.X_test.shape[0]} rows, {self.X_test.shape[1]} columns\")\n",
    "\n",
    "        # If y_train is not none, print its shape\n",
    "        if self.y_train is not None:\n",
    "            print(f\"Shape of y_train: {self.y_train.shape[0]} rows, 1 column\")\n",
    "\n",
    "        # If y_test is not none, print its shape\n",
    "        if self.y_test is not None:\n",
    "            print(f\"Shape of y_test: {self.y_test.shape[0]} rows, 1 column\")\n",
    "\n",
    "    # Define a function to make a scaled version of the full dataframe and the training and testing data if they exist\n",
    "    def scale(self, scaler=\"standard\", exclude_columns=[]):\n",
    "        \"\"\"\n",
    "        Scale the data using either standard or minmax scaling.\n",
    "    \n",
    "        Parameters:\n",
    "        - scaler: The type of scaling to use, either \"standard\" or \"minmax\".\n",
    "        - exclude_columns: A list of columns to exclude from scaling.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a standard and minmax scaler\n",
    "        standard_scaler = StandardScaler()\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "\n",
    "        # If the full dataframe is not none, scale it, but don't scale the columns in exclude_columns \n",
    "        if self.full_dataframe is not None:\n",
    "            if self.target is None:\n",
    "                if scaler == \"standard\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)] = standard_scaler.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)])\n",
    "                elif scaler == \"minmax\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)] = minmax_scaler.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)])\n",
    "            else:\n",
    "                if scaler == \"standard\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])] = standard_scaler.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])])\n",
    "                elif scaler == \"minmax\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])] = minmax_scaler.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])])\n",
    "\n",
    "\n",
    "        # If X_train is not none, scale it, but don't scale the columns in exclude_columns\n",
    "        if self.X_train is not None:\n",
    "            if scaler == \"standard\":\n",
    "                self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)] = standard_scaler.fit_transform(self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)])\n",
    "            elif scaler == \"minmax\":\n",
    "                self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)] = minmax_scaler.fit_transform(self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)])\n",
    "\n",
    "        # If X_test is not none, scale it, but don't scale the columns in exclude_columns \n",
    "        if self.X_test is not None:\n",
    "            if scaler == \"standard\":\n",
    "                self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)] = standard_scaler.transform(self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)])\n",
    "            elif scaler == \"minmax\":\n",
    "                self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)] = minmax_scaler.transform(self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)])\n",
    "\n",
    "    # Define a function to impute missing values in the full dataframe and the training and testing data if they exist\n",
    "    def impute(self, imputer=\"simple\", exclude_columns=[]):\n",
    "        \"\"\"\n",
    "        Impute missing values using either simple or KNN imputation.\n",
    "    \n",
    "        Parameters:\n",
    "        - imputer: The type of imputation to use, either \"simple\" or \"knn\".\n",
    "        - exclude_columns: A list of columns to exclude from imputation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a simple and KNN imputer\n",
    "        simple_imputer = SimpleImputer()\n",
    "        knn_imputer = KNNImputer()\n",
    "\n",
    "        # If the full dataframe is not none, impute it, but don't impute the columns in exclude_columns (however make sure they are still in the dataframe)\n",
    "        if self.full_dataframe is not None:\n",
    "            if self.target is None:\n",
    "                if imputer == \"simple\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)] = simple_imputer.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)])\n",
    "                elif imputer == \"knn\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)] = knn_imputer.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns)])\n",
    "            else:\n",
    "                if imputer == \"simple\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])] = simple_imputer.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])])\n",
    "                elif imputer == \"knn\":\n",
    "                    self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])] = knn_imputer.fit_transform(self.full_dataframe.loc[:, self.full_dataframe.columns.difference(exclude_columns+[self.target])])\n",
    "\n",
    "        # If X_train is not none, impute it, but don't impute the columns in exclude_columns (however make sure they are still in the dataframe)\n",
    "        if self.X_train is not None:\n",
    "            if imputer == \"simple\":\n",
    "                self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)] = simple_imputer.fit_transform(self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)])\n",
    "            elif imputer == \"knn\":\n",
    "                self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)] = knn_imputer.fit_transform(self.X_train.loc[:, self.X_train.columns.difference(exclude_columns)])\n",
    "\n",
    "        # If X_test is not none, impute it, but don't impute the columns in exclude_columns (however make sure they are still in the dataframe)\n",
    "        if self.X_test is not None:\n",
    "            if imputer == \"simple\":\n",
    "                self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)] = simple_imputer.transform(self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)])\n",
    "            elif imputer == \"knn\":\n",
    "                self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)] = knn_imputer.transform(self.X_test.loc[:, self.X_test.columns.difference(exclude_columns)])\n",
    "\n",
    "    # Define a function to fit model 1\n",
    "    def fit_model_1(self, custom_params={}):\n",
    "        \"\"\"\n",
    "        Fit a first model to the training data.\n",
    "    \n",
    "        Parameters:\n",
    "        - custom_params: A dictionary of custom parameters to use when fitting the model.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            self.model1 = LogisticRegression(**custom_params)\n",
    "        else:\n",
    "            self.model1 = LinearRegression(**custom_params)\n",
    "        self.model1.fit(self.X_train, self.y_train)\n",
    "\n",
    "    # Define a function to fit model 2\n",
    "    def fit_model_2(self, custom_params={}):\n",
    "        \"\"\"\n",
    "        Fit a second model to the training data.\n",
    "    \n",
    "        Parameters:\n",
    "        - custom_params: A dictionary of custom parameters to use when fitting the model.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            self.model2 = RandomForestClassifier(**custom_params)\n",
    "        else:\n",
    "            self.model2 = RandomForestRegressor(**custom_params)\n",
    "        self.model2.fit(self.X_train, self.y_train)\n",
    "\n",
    "    # Define a function to fit model 3\n",
    "    def fit_model_3(self, custom_params={}):\n",
    "        \"\"\"\n",
    "        Fit a third model to the training data.\n",
    "    \n",
    "        Parameters:\n",
    "        - custom_params: A dictionary of custom parameters to use when fitting the model.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            self.model3 = XGBClassifier(**custom_params)\n",
    "        else:\n",
    "            self.model3 = XGBRegressor(**custom_params)\n",
    "        self.model3.fit(self.X_train, self.y_train)\n",
    "\n",
    "    # Define a function to fit model 1 but using cv, but not grid search\n",
    "    def fit_model_1_cv(self, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a first model to the training data using cross-validation.\n",
    "    \n",
    "        Parameters:\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for cross-validation.\n",
    "        \"\"\"\n",
    "        scores = cross_val_score(self.model1, self.X_train, self.y_train, cv=cv, scoring=scoring)\n",
    "        print(f\"Cross-Validation Scores: {scores}\")\n",
    "        print(f\"Mean Cross-Validation Score: {scores.mean()}\")\n",
    "\n",
    "    # Define a function to fit model 2 but using cv, but not grid search\n",
    "    def fit_model_2_cv(self, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a second model to the training data using cross-validation.\n",
    "    \n",
    "        Parameters:\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for cross-validation.\n",
    "        \"\"\"\n",
    "        scores = cross_val_score(self.model2, self.X_train, self.y_train, cv=cv, scoring=scoring)\n",
    "        print(f\"Cross-Validation Scores: {scores}\")\n",
    "        print(f\"Mean Cross-Validation Score: {scores.mean()}\")\n",
    "\n",
    "    # Define a function to fit model 3 but using cv, but not grid search\n",
    "    def fit_model_3_cv(self, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a third model to the training data using cross-validation.\n",
    "    \n",
    "        Parameters:\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for cross-validation.\n",
    "        \"\"\"\n",
    "        scores = cross_val_score(self.model3, self.X_train, self.y_train, cv=cv, scoring=scoring)\n",
    "        print(f\"Cross-Validation Scores: {scores}\")\n",
    "        print(f\"Mean Cross-Validation Score: {scores.mean()}\")\n",
    "\n",
    "    # Define a function to fit model 1 but using cv and grid search to find the best scoring hyperparameters\n",
    "    def fit_model_1_cv_gs(self, param_grid={}, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a first model to the training data using cross-validation and grid search.\n",
    "    \n",
    "        Parameters:\n",
    "        - param_grid: A dictionary of hyperparameters to search over.\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for grid search.\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(self.model1, param_grid, cv=cv, scoring=scoring)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        print(f\"Best Score: {grid_search.best_score_}\")\n",
    "        print(f\"Best Grid Search Hyperparameters: {grid_search.best_params_}\")\n",
    "        self.model1 = grid_search.best_estimator_\n",
    "\n",
    "    # Define a function to fit model 2 but using cv and grid search to find the best scoring hyperparameters\n",
    "    def fit_model_2_cv_gs(self, param_grid={}, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a second model to the training data using cross-validation and grid search.\n",
    "    \n",
    "        Parameters:\n",
    "        - param_grid: A dictionary of hyperparameters to search over.\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for grid search.\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(self.model2, param_grid, cv=cv, scoring=scoring)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        print(f\"Best Score: {grid_search.best_score_}\")\n",
    "        print(f\"Best Grid Search Hyperparameters: {grid_search.best_params_}\")\n",
    "        self.model2 = grid_search.best_estimator_\n",
    "\n",
    "    # Define a function to fit model 3 but using cv and grid search to find the best scoring hyperparameters\n",
    "    def fit_model_3_cv_gs(self, param_grid={}, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Fit a third model to the training data using cross-validation and grid search.\n",
    "    \n",
    "        Parameters:\n",
    "        - param_grid: A dictionary of hyperparameters to search over.\n",
    "        - cv: The number of cross-validation folds to use.\n",
    "        - scoring: The scoring metric to use for grid search.\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(self.model3, param_grid, cv=cv, scoring=scoring)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        print(f\"Best Score: {grid_search.best_score_}\")\n",
    "        print(f\"Best Grid Search Hyperparameters: {grid_search.best_params_}\")\n",
    "        self.model3 = grid_search.best_estimator_\n",
    "\n",
    "    # Define a function to evaluate model 1 on the testing data\n",
    "    def evaluate_model_1(self):\n",
    "        \"\"\"\n",
    "        Evaluate the first model on the testing data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            y_pred = self.model1.predict(self.X_test)\n",
    "            print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Precision: {precision_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Recall: {recall_score(self.y_test, y_pred)}\")\n",
    "            print(f\"F1 Score: {f1_score(self.y_test, y_pred)}\")\n",
    "            print(f\"ROC AUC Score: {roc_auc_score(self.y_test, y_pred)}\")\n",
    "\n",
    "            # Show the classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "            # Plot the ROC curve\n",
    "            # Determine the number of classes first so we can plot the ROC curve accordingly\n",
    "            temp_n_classes = len(np.unique(self.y_test))\n",
    "            if temp_n_classes == 2:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, y_pred)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure()\n",
    "                lw = 2\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('Receiver Operating Characteristic Curve')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.show()\n",
    "        else:\n",
    "            y_pred = self.model1.predict(self.X_test)\n",
    "            print(f\"Mean Squared Error: {mean_squared_error(self.y_test, y_pred)}\")\n",
    "            print(f\"Mean Absolute Error: {mean_absolute_error(self.y_test, y_pred)}\")\n",
    "            print(f\"R2 Score: {r2_score(self.y_test, y_pred)}\")\n",
    "\n",
    "    # Define a function to evaluate model 2 on the testing data\n",
    "    def evaluate_model_2(self):\n",
    "        \"\"\"\n",
    "        Evaluate the second model on the testing data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            y_pred = self.model2.predict(self.X_test)\n",
    "            print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Precision: {precision_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Recall: {recall_score(self.y_test, y_pred)}\")\n",
    "            print(f\"F1 Score: {f1_score(self.y_test, y_pred)}\")\n",
    "            print(f\"ROC AUC Score: {roc_auc_score(self.y_test, y_pred)}\")\n",
    "\n",
    "            # Show the classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "            # Plot the ROC curve\n",
    "            # Determine the number of classes first so we can plot the ROC curve accordingly\n",
    "            temp_n_classes = len(np.unique(self.y_test))\n",
    "            if temp_n_classes == 2:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, y_pred)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure()\n",
    "                lw = 2\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('Receiver Operating Characteristic Curve')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.show()\n",
    "        else:\n",
    "            y_pred = self.model2.predict(self.X_test)\n",
    "            print(f\"Mean Squared Error: {mean_squared_error(self.y_test, y_pred)}\")\n",
    "            print(f\"Mean Absolute Error: {mean_absolute_error(self.y_test, y_pred)}\")\n",
    "            print(f\"R2 Score: {r2_score(self.y_test, y_pred)}\")\n",
    "\n",
    "    # Define a function to evaluate model 3 on the testing data\n",
    "    def evaluate_model_3(self):\n",
    "        \"\"\"\n",
    "        Evaluate the third model on the testing data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            y_pred = self.model3.predict(self.X_test)\n",
    "            print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Precision: {precision_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Recall: {recall_score(self.y_test, y_pred)}\")\n",
    "            print(f\"F1 Score: {f1_score(self.y_test, y_pred)}\")\n",
    "            print(f\"ROC AUC Score: {roc_auc_score(self.y_test, y_pred)}\")\n",
    "\n",
    "            # Show the classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "            # Plot the ROC curve\n",
    "            # Determine the number of classes first so we can plot the ROC curve accordingly\n",
    "            temp_n_classes = len(np.unique(self.y_test))\n",
    "            if temp_n_classes == 2:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, y_pred)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure()\n",
    "                lw = 2\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('Receiver Operating Characteristic Curve')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.show()\n",
    "        else:\n",
    "            y_pred = self.model3.predict(self.X_test)\n",
    "            print(f\"Mean Squared Error: {mean_squared_error(self.y_test, y_pred)}\")\n",
    "            print(f\"Mean Absolute Error: {mean_absolute_error(self.y_test, y_pred)}\")\n",
    "            print(f\"R2 Score: {r2_score(self.y_test, y_pred)}\")\n",
    "\n",
    "    # Define a function to fit ensemble 1 to the training data\n",
    "    def fit_ensemble_1(self):\n",
    "        \"\"\"\n",
    "        Fit the first ensemble to the training data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            self.Ensemble1 = VotingClassifier(estimators=[('lr', self.model1), ('rf', self.model2), ('xgb', self.model3)])\n",
    "        else:\n",
    "            self.Ensemble1 = VotingRegressor(estimators=[('lr', self.model1), ('rf', self.model2), ('xgb', self.model3)])\n",
    "        self.Ensemble1.fit(self.X_train, self.y_train)\n",
    "\n",
    "    # Define a function to fit ensemble 2 to the training data\n",
    "    def fit_ensemble_2(self):\n",
    "        \"\"\"\n",
    "        Fit the second ensemble to the training data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            self.Ensemble2 = StackingClassifier(estimators=[('lr', self.model1), ('rf', self.model2), ('xgb', self.model3)])\n",
    "        else:\n",
    "            self.Ensemble2 = StackingRegressor(estimators=[('lr', self.model1), ('rf', self.model2), ('xgb', self.model3)])\n",
    "        self.Ensemble2.fit(self.X_train, self.y_train)\n",
    "\n",
    "    # Define a function to evaluate ensemble 1 on the testing data\n",
    "    def evaluate_ensemble_1(self):\n",
    "        \"\"\"\n",
    "        Evaluate the first ensemble on the testing data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            y_pred = self.Ensemble1.predict(self.X_test)\n",
    "            print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Precision: {precision_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Recall: {recall_score(self.y_test, y_pred)}\")\n",
    "            print(f\"F1 Score: {f1_score(self.y_test, y_pred)}\")\n",
    "            print(f\"ROC AUC Score: {roc_auc_score(self.y_test, y_pred)}\")\n",
    "        else:\n",
    "            y_pred = self.Ensemble1.predict(self.X_test)\n",
    "            print(f\"Mean Squared Error: {mean_squared_error(self.y_test, y_pred)}\")\n",
    "            print(f\"Mean Absolute Error: {mean_absolute_error(self.y_test, y_pred)}\")\n",
    "            print(f\"R2 Score: {r2_score(self.y_test, y_pred)}\")\n",
    "\n",
    "    # Define a function to evaluate ensemble 2 on the testing data\n",
    "    def evaluate_ensemble_2(self):\n",
    "        \"\"\"\n",
    "        Evaluate the second ensemble on the testing data.\n",
    "        \"\"\"\n",
    "        if self.problem_type == 'classification':\n",
    "            y_pred = self.Ensemble2.predict(self.X_test)\n",
    "            print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Precision: {precision_score(self.y_test, y_pred)}\")\n",
    "            print(f\"Recall: {recall_score(self.y_test, y_pred)}\")\n",
    "            print(f\"F1 Score: {f1_score(self.y_test, y_pred)}\")\n",
    "            print(f\"ROC AUC Score: {roc_auc_score(self.y_test, y_pred)}\")\n",
    "        else:\n",
    "            y_pred = self.Ensemble2.predict(self.X_test)\n",
    "            print(f\"Mean Squared Error: {mean_squared_error(self.y_test, y_pred)}\")\n",
    "            print(f\"Mean Absolute Error: {mean_absolute_error(self.y_test, y_pred)}\")\n",
    "            print(f\"R2 Score: {r2_score(self.y_test, y_pred)}\")\n",
    "\n",
    "    # Define a function for checking for missing values\n",
    "    def missing(self):\n",
    "        \"\"\"\n",
    "        Check for missing values in the full dataframe and the training and testing data if they exist.\n",
    "        \"\"\"\n",
    "        if self.full_dataframe is not None:\n",
    "            print(\"Missing Values in Full Dataframe:\")\n",
    "            print(self.full_dataframe.isnull().sum())\n",
    "\n",
    "        if self.X_train is not None:\n",
    "            print(\"\\nMissing Values in X_train:\")\n",
    "            print(self.X_train.isnull().sum())\n",
    "\n",
    "        if self.X_test is not None:\n",
    "            print(\"\\nMissing Values in X_test:\")\n",
    "            print(self.X_test.isnull().sum())\n",
    "            \n",
    "        if self.y_train is not None:\n",
    "            print(\"\\nMissing Values in y_train:\")\n",
    "            print(self.y_train.isnull().sum())\n",
    "            \n",
    "        if self.y_test is not None:\n",
    "            print(\"\\nMissing Values in y_test:\")\n",
    "            print(self.y_test.isnull().sum())\n",
    "\n",
    "    # Define a function for making a simple, interactive histogram.\n",
    "    def histogram(self, variable):\n",
    "        \"\"\"\n",
    "        Create a simple, interactive histogram with different colored bars using plotly.\n",
    "    \n",
    "        Parameters:\n",
    "        - variable: The variable to plot the histogram for.\n",
    "        \"\"\"\n",
    "        # Create a histogram for the variable in the full dataframe\n",
    "        if self.full_dataframe is not None:\n",
    "            fig = px.histogram(self.full_dataframe, x=variable, title=f\"Histogram of {variable} for full dataframe\")\n",
    "            fig.show()\n",
    "\n",
    "        # Create a histogram for the variable in the training data\n",
    "        if self.X_train is not None:\n",
    "            fig = px.histogram(self.X_train, x=variable, title=f\"Histogram of {variable} for training data\")\n",
    "            fig.show()\n",
    "\n",
    "        # Create a histogram for the variable in the testing data\n",
    "        if self.X_test is not None:\n",
    "            fig = px.histogram(self.X_test, x=variable, title=f\"Histogram of {variable} for testing data\")\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "    # Define a function for making a simple, interactive scatter plot.\n",
    "    def scatter(self, x, y):\n",
    "        \"\"\"\n",
    "        Create a simple, interactive scatter plot with different colored points using plotly.\n",
    "    \n",
    "        Parameters:\n",
    "        - x: The variable to plot on the x-axis.\n",
    "        - y: The variable to plot on the y-axis.\n",
    "        \"\"\"\n",
    "        # Create a scatter plot for the full dataframe\n",
    "        if self.full_dataframe is not None:\n",
    "            if self.target is not None:\n",
    "                fig = px.scatter(self.full_dataframe, x=x, y=y, color=self.target, title=f\"Scatter plot of {x} vs {y} for full dataframe\")\n",
    "            else:\n",
    "                fig = px.scatter(self.full_dataframe, x=x, y=y, title=f\"Scatter plot of {x} vs {y} for full dataframe\")\n",
    "            fig.show()\n",
    "\n",
    "        # Create a scatter plot for the training data\n",
    "        if self.X_train is not None:\n",
    "            if self.target is not None:\n",
    "                fig = px.scatter(self.X_train, x=x, y=y, color=self.y_train, title=f\"Scatter plot of {x} vs {y} for training data\")\n",
    "            else:\n",
    "                fig = px.scatter(self.X_train, x=x, y=y, title=f\"Scatter plot of {x} vs {y} for training data\")\n",
    "            fig.show()\n",
    "\n",
    "        # Create a scatter plot for the testing data\n",
    "        if self.X_test is not None:\n",
    "            if self.target is not None:\n",
    "                fig = px.scatter(self.X_test, x=x, y=y, color=self.y_test, title=f\"Scatter plot of {x} vs {y} for testing data\")\n",
    "            else:\n",
    "                fig = px.scatter(self.X_test, x=x, y=y, title=f\"Scatter plot of {x} vs {y} for testing data\")\n",
    "            fig.show()\n",
    "\n",
    "    # Define a function to round all feature values in the full dataframe to a specified number of decimal places\n",
    "    def round(self, decimals=2):\n",
    "        \"\"\"\n",
    "        Round all values in the full dataframe to a specified number of decimal places.\n",
    "    \n",
    "        Parameters:\n",
    "        - decimals: The number of decimal places to round to.\n",
    "        \"\"\"\n",
    "        if self.full_dataframe is not None:\n",
    "            self.full_dataframe = self.full_dataframe.round(decimals)\n",
    "\n",
    "        if self.X_train is not None:\n",
    "            self.X_train = self.X_train.round(decimals)\n",
    "\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.X_test.round(decimals)\n",
    "\n",
    "        if self.y_train is not None:\n",
    "            self.y_train = self.y_train.round(decimals)\n",
    "\n",
    "        if self.y_test is not None:\n",
    "            self.y_test = self.y_test.round(decimals)\n",
    "\n",
    "    # Define a function for common transforms like boxcox, log, sqrt, etc.\n",
    "    def transform(self, method='boxcox', columns=[], custom_params={}):\n",
    "        \"\"\"\n",
    "        Transform the data using a specified method.\n",
    "    \n",
    "        Parameters:\n",
    "        - method: The transformation method to use.\n",
    "        - columns: A list of columns to transform.\n",
    "        - custom_params: A dictionary of custom parameters to use when transforming the data.\n",
    "        \"\"\"\n",
    "\n",
    "        if method == 'boxcox':\n",
    "            transformer = PowerTransformer(method='box-cox', **custom_params)\n",
    "        elif method == 'log':\n",
    "            transformer = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=True)\n",
    "        elif method == 'sqrt':\n",
    "            transformer = FunctionTransformer(func=np.sqrt, inverse_func=np.square, validate=True)\n",
    "        elif method == 'cube':\n",
    "            transformer = FunctionTransformer(func=np.cbrt, inverse_func=lambda x: x**3, validate=True)\n",
    "        elif method == 'yeo-johnson':\n",
    "            transformer = PowerTransformer(method='yeo-johnson', **custom_params)\n",
    "\n",
    "        # Apply the transformation to the columns in the full dataframe\n",
    "        if self.full_dataframe is not None:\n",
    "            self.full_dataframe.loc[:, columns] = transformer.fit_transform(self.full_dataframe.loc[:, columns])\n",
    "\n",
    "        # Apply the transformation to the columns in the training data\n",
    "        if self.X_train is not None:\n",
    "            self.X_train.loc[:, columns] = transformer.fit_transform(self.X_train.loc[:, columns])\n",
    "\n",
    "        # Apply the transformation to the columns in the testing data\n",
    "        if self.X_test is not None:\n",
    "            self.X_test.loc[:, columns] = transformer.transform(self.X_test.loc[:, columns])\n",
    "\n",
    "    # Define a function to remove outliers\n",
    "    def outliers(self, method='zscore', apply_to_test_data=True, custom_params={}):\n",
    "        \"\"\"\n",
    "        Remove outliers from the data using a specified method, while ensuring corresponding rows in target data are also removed.\n",
    "        \"\"\"\n",
    "        # Define a helper function for filtering using IQR and capturing indices\n",
    "        def filter_iqr(df, Q1, Q3, IQR):\n",
    "            # Ensure alignment before condition calculation\n",
    "            aligned_df, _ = df.align(Q1, axis=1, join='inner', copy=False)\n",
    "            condition = ((aligned_df < (Q1 - 1.5 * IQR)) | (aligned_df > (Q3 + 1.5 * IQR)))\n",
    "            filtered_df = df[~condition.any(axis=1)]\n",
    "            return filtered_df, filtered_df.index\n",
    "\n",
    "        # Log the number of rows before outlier removal\n",
    "        temp_n_rows_full = self.full_dataframe.shape[0] if self.full_dataframe is not None else 0\n",
    "        temp_n_rows_x_train = self.X_train.shape[0] if self.X_train is not None else 0\n",
    "        temp_n_rows_x_test = self.X_test.shape[0] if self.X_test is not None else 0\n",
    "        temp_n_rows_y_train = self.y_train.shape[0] if self.y_train is not None else 0\n",
    "        temp_n_rows_y_test = self.y_test.shape[0] if self.y_test is not None else 0\n",
    "\n",
    "        # Z-Score method\n",
    "        if method == 'zscore':\n",
    "            if custom_params == {}:\n",
    "                custom_params = {'threshold': 3}\n",
    "            # Full dataframe\n",
    "            if self.full_dataframe is not None:\n",
    "                # if the target is none\n",
    "                if self.target is None:\n",
    "                    z = np.abs(zscore(self.full_dataframe))\n",
    "                    self.full_dataframe = self.full_dataframe[(z < custom_params['threshold']).all(axis=1)]\n",
    "                    # if the target exists\n",
    "                else:\n",
    "                    z = np.abs(zscore(self.full_dataframe.drop(columns=[self.target]) if self.target else self.full_dataframe))\n",
    "                    self.full_dataframe = self.full_dataframe[(z < custom_params['threshold']).all(axis=1)]\n",
    "            # X_train\n",
    "            if self.X_train is not None:\n",
    "                z = np.abs(zscore(self.X_train))\n",
    "                retained_indices = self.X_train.index[(z < custom_params['threshold']).all(axis=1)]\n",
    "                self.X_train = self.X_train.loc[retained_indices]\n",
    "                self.y_train = self.y_train.loc[retained_indices]\n",
    "            # X_test\n",
    "            if self.X_test is not None and apply_to_test_data:\n",
    "                z = np.abs(zscore(self.X_test))\n",
    "                retained_indices = self.X_test.index[(z < custom_params['threshold']).all(axis=1)]\n",
    "                self.X_test = self.X_test.loc[retained_indices]\n",
    "                if hasattr(self, 'y_test'):  # Check if y_test exists\n",
    "                    self.y_test = self.y_test.loc[retained_indices]\n",
    "\n",
    "        # IQR method\n",
    "        elif method == 'iqr':\n",
    "            if custom_params == {}:\n",
    "                custom_params = {'q1': 0.25, 'q3': 0.75}\n",
    "            # Full dataframe\n",
    "            if self.full_dataframe is not None:\n",
    "                # Temporarily remove target column if it exists\n",
    "                df_temp = self.full_dataframe.drop(columns=[self.target]) if self.target else self.full_dataframe.copy()\n",
    "                Q1 = df_temp.quantile(custom_params['q1'])\n",
    "                Q3 = df_temp.quantile(custom_params['q3'])\n",
    "                IQR = Q3 - Q1\n",
    "                # Filter using modified dataframe but ensure the original dataframe is updated correctly\n",
    "                filtered_df, _ = filter_iqr(df_temp, Q1, Q3, IQR)\n",
    "                # If the target was removed, use the indices to select rows from the original dataframe\n",
    "                self.full_dataframe = self.full_dataframe.loc[filtered_df.index] if self.target else filtered_df\n",
    "            # X_train\n",
    "            if self.X_train is not None:\n",
    "                Q1 = self.X_train.quantile(custom_params['q1'])\n",
    "                Q3 = self.X_train.quantile(custom_params['q3'])\n",
    "                IQR = Q3 - Q1\n",
    "                self.X_train, retained_indices = filter_iqr(self.X_train, Q1, Q3, IQR)\n",
    "                self.y_train = self.y_train.loc[retained_indices]\n",
    "            # X_test\n",
    "            if self.X_test is not None and apply_to_test_data:\n",
    "                Q1 = self.X_test.quantile(custom_params['q1'])\n",
    "                Q3 = self.X_test.quantile(custom_params['q3'])\n",
    "                IQR = Q3 - Q1\n",
    "                self.X_test, retained_indices = filter_iqr(self.X_test, Q1, Q3, IQR)\n",
    "                if hasattr(self, 'y_test'):  # Check if y_test exists\n",
    "                    self.y_test = self.y_test.loc[retained_indices]\n",
    "\n",
    "        # Isolation Forest method\n",
    "        elif method == 'isolation':\n",
    "            if custom_params == {}:\n",
    "                custom_params = {'contamination': 0.1}\n",
    "            # Full dataframe\n",
    "            if self.full_dataframe is not None:\n",
    "                # if the target is none\n",
    "                if self.target is None:\n",
    "                    isolation_forest = IsolationForest(contamination=custom_params['contamination'])\n",
    "                    self.full_dataframe = self.full_dataframe[isolation_forest.fit_predict(self.full_dataframe) == 1]\n",
    "                # if the target exists\n",
    "                else:\n",
    "                    isolation_forest = IsolationForest(contamination=custom_params['contamination'])\n",
    "                    self.full_dataframe = self.full_dataframe[isolation_forest.fit_predict(self.full_dataframe.drop(columns=[self.target])) == 1]\n",
    "            # X_train\n",
    "            if self.X_train is not None:\n",
    "                isolation_forest = IsolationForest(contamination=custom_params['contamination'])\n",
    "                retained_indices = isolation_forest.fit_predict(self.X_train) == 1\n",
    "                self.X_train = self.X_train.loc[retained_indices]\n",
    "                self.y_train = self.y_train.loc[retained_indices]\n",
    "            # X_test\n",
    "            if self.X_test is not None and apply_to_test_data:\n",
    "                isolation_forest = IsolationForest(contamination=custom_params['contamination'])\n",
    "                retained_indices = isolation_forest.fit_predict(self.X_test) == 1\n",
    "                self.X_test = self.X_test.loc[retained_indices]\n",
    "                if hasattr(self, 'y_test'):  # Check if y_test exists\n",
    "                    self.y_test = self.y_test.loc[retained_indices]\n",
    "\n",
    "        # Local Outlier Factor method\n",
    "        elif method == 'lof':\n",
    "            if custom_params == {}:\n",
    "                custom_params = {'contamination': 0.1}\n",
    "            # Full dataframe\n",
    "            if self.full_dataframe is not None:\n",
    "                # if the target is none\n",
    "                if self.target is None:\n",
    "                    lof = LocalOutlierFactor(contamination=custom_params['contamination'])\n",
    "                    self.full_dataframe = self.full_dataframe[lof.fit_predict(self.full_dataframe) == 1]\n",
    "                # if the target exists\n",
    "                else:\n",
    "                    lof = LocalOutlierFactor(contamination=custom_params['contamination'])\n",
    "                    self.full_dataframe = self.full_dataframe[lof.fit_predict(self.full_dataframe.drop(columns=[self.target])) == 1]\n",
    "            # X_train\n",
    "            if self.X_train is not None:\n",
    "                lof = LocalOutlierFactor(contamination=custom_params['contamination'])\n",
    "                retained_indices = lof.fit_predict(self.X_train) == 1\n",
    "                self.X_train = self.X_train.loc[retained_indices]\n",
    "                self.y_train = self.y_train.loc[retained_indices]\n",
    "            # X_test\n",
    "            if self.X_test is not None and apply_to_test_data:\n",
    "                lof = LocalOutlierFactor(contamination=custom_params['contamination'])\n",
    "                retained_indices = lof.fit_predict(self.X_test) == 1\n",
    "                self.X_test = self.X_test.loc[retained_indices]\n",
    "                if hasattr(self, 'y_test'):  # Check if y_test exists\n",
    "                    self.y_test = self.y_test.loc[retained_indices]\n",
    "\n",
    "        # Determine and print the number of rows removed from each dataset\n",
    "        print(f\"Number of Rows Removed from Full Dataframe: {temp_n_rows_full - self.full_dataframe.shape[0] if self.full_dataframe is not None else 0}\")\n",
    "        print(f\"Number of Rows Removed from X_train: {temp_n_rows_x_train - self.X_train.shape[0] if self.X_train is not None else 0}\")\n",
    "        print(f\"Number of Rows Removed from X_test: {temp_n_rows_x_test - self.X_test.shape[0] if self.X_test is not None else 0}\")\n",
    "        print(f\"Number of Rows Removed from y_train: {temp_n_rows_y_train - self.y_train.shape[0] if self.y_train is not None else 0}\")\n",
    "        print(f\"Number of Rows Removed from y_test: {temp_n_rows_y_test - self.y_test.shape[0] if self.y_test is not None else 0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
